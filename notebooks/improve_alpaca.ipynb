{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Improve Norwegian Alpaca Dataset with Borealis\n",
    "\n",
    "This notebook uses **NbAiLab/borealis-4b-instruct-preview** (a Norwegian-focused Gemma 3 4B fine-tune) to improve the Norwegian text columns in the **NbAiLab/norwegian-alpaca** dataset.\n",
    "\n",
    "The original dataset was machine-translated from Stanford Alpaca via gpt-3.5-turbo and contains translation artifacts, unnatural phrasing, and inconsistent quality. Borealis rewrites the three Norwegian columns (`instruction`, `input`, `output`) into natural, fluent Norwegian Bokmål while preserving the original meaning.\n",
    "\n",
    "**Dataset:** 51,942 rows, 6 columns (3 Norwegian + 3 English)\n",
    "\n",
    "**Pipeline:**\n",
    "1. Load the dataset from HuggingFace\n",
    "2. For each Norwegian text field, prompt Borealis with a few-shot Norwegian refinement prompt\n",
    "3. Validate responses (strip preambles, check for hallucination, length sanity)\n",
    "4. Save improved dataset with both original and improved columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library (editable mode)\n",
    "# On Colab, clone the repo first:\n",
    "#   !git clone https://github.com/your-username/NORAI-Tools.git /content/NORAI-Tools\n",
    "#   %pip install -e /content/NORAI-Tools\n",
    "\n",
    "%pip install -e ..\n",
    "\n",
    "from norai_tools import (\n",
    "    AlpacaImprover,\n",
    "    load_alpaca,\n",
    "    save_improved,\n",
    "    DEFAULT_MODEL,\n",
    "    DEFAULT_BATCH_SIZE,\n",
    "    DEFAULT_CHECKPOINT_EVERY,\n",
    "    CHECKPOINT_FILE,\n",
    "    OUTPUT_FILE,\n",
    ")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "\n",
    "MODEL_NAME = DEFAULT_MODEL\n",
    "DEVICE = \"auto\"              # \"auto\" lets accelerate pick GPU/MPS/CPU\n",
    "BATCH_SIZE = DEFAULT_BATCH_SIZE\n",
    "CHECKPOINT_EVERY = DEFAULT_CHECKPOINT_EVERY\n",
    "\n",
    "# Optional: HuggingFace Hub push\n",
    "PUSH_TO_HUB = False\n",
    "HUB_REPO_ID = \"your-username/norwegian-alpaca-improved\"  # Change this\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Checkpoint every: {CHECKPOINT_EVERY} rows\")\n",
    "print(f\"Output: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load dataset from HuggingFace\n",
    "# ============================================================\n",
    "\n",
    "dataset = load_alpaca()\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset)} rows\")\n",
    "print(f\"Columns: {dataset.column_names}\")\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "display(\n",
    "    dataset.select(range(5))\n",
    "    .to_pandas()[[\"instruction\", \"instruction_en\", \"input\", \"output\"]]\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load Borealis model\n",
    "# ============================================================\n",
    "\n",
    "improver = AlpacaImprover(\n",
    "    model_name=MODEL_NAME,\n",
    "    device=DEVICE,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "improver.load_model()\n",
    "\n",
    "print(f\"Model loaded: {improver.model_name}\")\n",
    "print(f\"Dtype: {next(improver.model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Run improvement loop (with checkpoint support)\n",
    "# ============================================================\n",
    "\n",
    "# To test on a small subset first, uncomment:\n",
    "# dataset = dataset.select(range(100))\n",
    "\n",
    "improved_dataset = improver.improve_dataset(\n",
    "    dataset,\n",
    "    checkpoint_path=CHECKPOINT_FILE,\n",
    "    checkpoint_every=CHECKPOINT_EVERY,\n",
    ")\n",
    "\n",
    "print(f\"\\nProcessed {len(improved_dataset)} rows.\")\n",
    "print(f\"Columns: {improved_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Inspect results — side-by-side comparison + change stats\n",
    "# ============================================================\n",
    "\n",
    "results_df = improved_dataset.to_pandas()\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"INSTRUCTION: Original vs Improved\")\n",
    "print(\"=\" * 80)\n",
    "for i in range(min(5, len(results_df))):\n",
    "    row = results_df.iloc[i]\n",
    "    print(f\"\\n--- Row {i} ---\")\n",
    "    print(f\"  ORIGINAL:  {row['instruction']}\")\n",
    "    print(f\"  IMPROVED:  {row['instruction_improved']}\")\n",
    "    print(f\"  ENGLISH:   {row['instruction_en']}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"OUTPUT: Original vs Improved\")\n",
    "print(\"=\" * 80)\n",
    "for i in range(min(5, len(results_df))):\n",
    "    row = results_df.iloc[i]\n",
    "    print(f\"\\n--- Row {i} ---\")\n",
    "    print(f\"  ORIGINAL:  {row['output'][:200]}\")\n",
    "    print(f\"  IMPROVED:  {row['output_improved'][:200]}\")\n",
    "    print(f\"  ENGLISH:   {row['output_en'][:200]}\")\n",
    "\n",
    "# Change statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Change Statistics\")\n",
    "print(\"=\" * 80)\n",
    "for col in [\"instruction\", \"input\", \"output\"]:\n",
    "    changed = (results_df[col] != results_df[f\"{col}_improved\"]).sum()\n",
    "    total = len(results_df)\n",
    "    print(f\"  {col}: {changed}/{total} changed ({100*changed/total:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Save output to parquet + optional Hub push\n",
    "# ============================================================\n",
    "\n",
    "save_improved(\n",
    "    improved_dataset,\n",
    "    path=OUTPUT_FILE,\n",
    "    push_to_hub=PUSH_TO_HUB,\n",
    "    hub_repo=HUB_REPO_ID if PUSH_TO_HUB else None,\n",
    ")\n",
    "\n",
    "print(f\"Saved to: {OUTPUT_FILE}\")\n",
    "print(f\"  Rows: {len(improved_dataset)}\")\n",
    "print(f\"  Columns: {improved_dataset.column_names}\")\n",
    "\n",
    "if not PUSH_TO_HUB:\n",
    "    print(\"\\nSet PUSH_TO_HUB = True and update HUB_REPO_ID to push to the Hub.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
