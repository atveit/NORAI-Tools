{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2b: GSPO Training on Qwen-Distilled Dataset\n",
    "\n",
    "This notebook performs **GSPO alignment training** on **Qwen/Qwen3.5-35B-A3B** using the Qwen-distilled + Borealis-polished dataset from Phase 1b.\n",
    "\n",
    "## How this differs from the standard GSPO notebook\n",
    "\n",
    "| Aspect | Standard (Phase 2) | Distilled (Phase 2b) |\n",
    "|--------|--------------------|----------------------|\n",
    "| Reference outputs | gpt-3.5-turbo, improved by Borealis | Qwen-generated, polished by Borealis |\n",
    "| Content quality | Mediocre (gpt-3.5 era) | High (Qwen3.5 knowledge) |\n",
    "| Norwegian quality | Good (Borealis-improved) | Good (Borealis-polished) |\n",
    "| Training signal | Learns from weaker reference | Self-distillation — refines its own outputs |\n",
    "\n",
    "Using Qwen's own outputs as reference creates a **self-distillation** loop: the model is rewarded for producing outputs similar to its best (polished) generations. This focuses GSPO on Norwegian fluency rather than content improvement.\n",
    "\n",
    "## GSPO Recap\n",
    "\n",
    "GSPO uses `importance_sampling_level=\"sequence\"` in TRL's `GRPOTrainer` for stable MoE training with sequence-level importance ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library with GSPO extras\n",
    "# On Colab:\n",
    "#   !git clone https://github.com/your-username/NORAI-Tools.git /content/NORAI-Tools\n",
    "#   %pip install -e \"/content/NORAI-Tools[gspo]\"\n",
    "\n",
    "%pip install -e \"../[gspo]\"\n",
    "%pip install trl>=0.28.0 peft>=0.15.0 accelerate bitsandbytes\n",
    "\n",
    "from norai_tools import (\n",
    "    semantic_reward,\n",
    "    language_reward,\n",
    "    length_reward,\n",
    "    accuracy_reward,\n",
    "    prepare_gspo_dataset,\n",
    "    validate_gspo_dataset,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "\n",
    "# Model\n",
    "MODEL_NAME = \"Qwen/Qwen3.5-35B-A3B\"\n",
    "\n",
    "# Dataset (from Phase 1b distillation notebook)\n",
    "DISTILLED_DATASET_PATH = \"norwegian_alpaca_qwen_polished.parquet\"\n",
    "\n",
    "# GSPO-specific settings\n",
    "IMPORTANCE_SAMPLING_LEVEL = \"sequence\"\n",
    "LOSS_TYPE = \"grpo\"\n",
    "BETA = 0.04\n",
    "EPSILON = 3e-4\n",
    "NUM_GENERATIONS = 8\n",
    "\n",
    "# Training hyperparams\n",
    "LEARNING_RATE = 5e-7\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "MAX_COMPLETION_LENGTH = 512\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "LOGGING_STEPS = 10\n",
    "SAVE_STEPS = 200\n",
    "BF16 = True\n",
    "USE_VLLM = True\n",
    "VLLM_MODE = \"colocate\"\n",
    "\n",
    "# LoRA config\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "\n",
    "# Reward weights: [semantic, language, length, accuracy]\n",
    "REWARD_WEIGHTS = [2.0, 1.0, 0.5, 1.5]\n",
    "\n",
    "# Output\n",
    "OUTPUT_DIR = \"./gspo_qwen_norwegian_distilled\"\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Dataset: {DISTILLED_DATASET_PATH}\")\n",
    "print(f\"GSPO: importance_sampling_level={IMPORTANCE_SAMPLING_LEVEL}, beta={BETA}, epsilon={EPSILON}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load distilled dataset, map columns, prepare for GSPO\n",
    "# ============================================================\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "dataset = load_dataset(\"parquet\", data_files=DISTILLED_DATASET_PATH, split=\"train\")\n",
    "print(f\"Loaded: {len(dataset)} rows\")\n",
    "print(f\"Columns: {dataset.column_names}\")\n",
    "\n",
    "# Verify distilled columns exist\n",
    "required = [\"output_qwen_polished\", \"instruction_improved\", \"input_improved\", \"instruction_en\"]\n",
    "missing = [c for c in required if c not in dataset.column_names]\n",
    "if missing:\n",
    "    raise ValueError(\n",
    "        f\"Missing columns: {missing}. \"\n",
    "        \"Run Phase 1b (distill_qwen_norwegian.ipynb) first.\"\n",
    "    )\n",
    "\n",
    "# Map output_qwen_polished → output_improved for reward function compatibility.\n",
    "# Keep the original Phase 1 output_improved as output_improved_phase1 for reference.\n",
    "if \"output_improved\" in dataset.column_names:\n",
    "    dataset = dataset.rename_column(\"output_improved\", \"output_improved_phase1\")\n",
    "dataset = dataset.rename_column(\"output_qwen_polished\", \"output_improved\")\n",
    "print(\"\\nRenamed: output_qwen_polished → output_improved (for reward functions)\")\n",
    "\n",
    "# Prepare GSPO columns (prompt + task_type)\n",
    "dataset = prepare_gspo_dataset(dataset)\n",
    "\n",
    "# Validate\n",
    "validation = validate_gspo_dataset(dataset)\n",
    "print(f\"\\nValid: {validation['is_valid']}\")\n",
    "print(f\"Empty prompts: {validation['empty_prompts']}\")\n",
    "\n",
    "# Task type distribution\n",
    "print(\"\\nTask type distribution:\")\n",
    "for task, count in sorted(validation[\"task_type_distribution\"].items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {task:20s}: {count:6d} ({100*count/len(dataset):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Pre-load semantic similarity model\n",
    "# ============================================================\n",
    "\n",
    "print(\"Loading semantic similarity model (intfloat/multilingual-e5-large)...\")\n",
    "_ = semantic_reward(\n",
    "    completions=[[{\"content\": \"test\"}]],\n",
    "    output_improved=[\"test\"],\n",
    ")\n",
    "print(\"Similarity model loaded.\")\n",
    "\n",
    "print(\"\\nReward functions:\")\n",
    "names = [\"semantic_reward\", \"language_reward\", \"length_reward\", \"accuracy_reward\"]\n",
    "for name, weight in zip(names, REWARD_WEIGHTS):\n",
    "    print(f\"  {name:20s}  weight={weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Setup LoRA + GRPOConfig with GSPO settings\n",
    "# ============================================================\n",
    "\n",
    "from peft import LoraConfig\n",
    "from trl import GRPOConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    # GSPO-specific\n",
    "    importance_sampling_level=IMPORTANCE_SAMPLING_LEVEL,\n",
    "    loss_type=LOSS_TYPE,\n",
    "    beta=BETA,\n",
    "    epsilon=EPSILON,\n",
    "    # Generation\n",
    "    num_generations=NUM_GENERATIONS,\n",
    "    max_completion_length=MAX_COMPLETION_LENGTH,\n",
    "    # Training\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    bf16=BF16,\n",
    "    # vLLM\n",
    "    use_vllm=USE_VLLM,\n",
    "    vllm_mode=VLLM_MODE,\n",
    "    # Logging/saving\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=3,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"LoRA config:\")\n",
    "print(f\"  r={peft_config.r}, alpha={peft_config.lora_alpha}, dropout={peft_config.lora_dropout}\")\n",
    "print(f\"  Target modules: {peft_config.target_modules}\")\n",
    "print()\n",
    "print(\"GSPO training config:\")\n",
    "print(f\"  importance_sampling_level: {training_args.importance_sampling_level}\")\n",
    "print(f\"  beta={training_args.beta}, epsilon={training_args.epsilon}\")\n",
    "print(f\"  num_generations={training_args.num_generations}\")\n",
    "print(f\"  lr={training_args.learning_rate}, batch={training_args.per_device_train_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Initialize GRPOTrainer and train\n",
    "# ============================================================\n",
    "\n",
    "from trl import GRPOTrainer\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=MODEL_NAME,\n",
    "    reward_funcs=[\n",
    "        semantic_reward,\n",
    "        language_reward,\n",
    "        length_reward,\n",
    "        accuracy_reward,\n",
    "    ],\n",
    "    reward_weights=REWARD_WEIGHTS,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "print(f\"Trainer initialized. Dataset: {len(dataset)} rows\")\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Training complete!\")\n",
    "print(f\"  Total steps: {train_result.global_step}\")\n",
    "print(f\"  Training loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Save LoRA adapter, print metrics, generate samples\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "print(f\"Model saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "metrics = train_result.metrics\n",
    "print(\"\\nTraining metrics:\")\n",
    "for key, value in sorted(metrics.items()):\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Sample Norwegian outputs\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Sample generations:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "eval_prompts = [\n",
    "    \"Forklar hva fotosyntese er med enkle ord.\",\n",
    "    \"Skriv tre fordeler med regelmessig trening.\",\n",
    "    \"Hva er forskjellen mellom vær og klima?\",\n",
    "]\n",
    "\n",
    "for prompt_text in eval_prompts:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(trainer.model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = trainer.model.generate(\n",
    "            **inputs, max_new_tokens=256, do_sample=True,\n",
    "            temperature=0.7, top_p=0.9,\n",
    "        )\n",
    "\n",
    "    generated = tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True\n",
    "    )\n",
    "    print(f\"\\nPrompt: {prompt_text}\")\n",
    "    print(f\"Response: {generated[:300]}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(f\"\\nLoRA adapter saved at: {OUTPUT_DIR}\")\n",
    "print(\"Load with: PeftModel.from_pretrained(base_model, '{}')\".format(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware Requirements & Tips\n",
    "\n",
    "Same requirements as the standard GSPO notebook (Phase 2).\n",
    "\n",
    "| Setup | Configuration | Notes |\n",
    "|-------|---------------|-------|\n",
    "| **Minimum** | 1x A100 80GB | LoRA + 4-bit quant + vLLM colocate, G=4 |\n",
    "| **Recommended** | 2x A100 80GB | LoRA + BF16, vLLM server on 2nd GPU, G=8 |\n",
    "| **Comfortable** | 4x A100 80GB | ZeRO Stage 3 + vLLM, G=16, larger batches |\n",
    "\n",
    "## Self-Distillation Notes\n",
    "\n",
    "Since the reference outputs were generated by the same Qwen model being trained:\n",
    "\n",
    "- **Semantic similarity rewards will be high from the start** — the model already \"knows\" how to produce similar content\n",
    "- **Language reward is the key differentiator** — it pushes the model toward more natural Norwegian\n",
    "- Consider **increasing the language reward weight** (e.g., `[1.5, 2.0, 0.5, 1.0]`) to emphasize Norwegian fluency\n",
    "- Monitor reward curves — if all rewards plateau quickly, reduce learning rate or increase epsilon\n",
    "\n",
    "## References\n",
    "\n",
    "- [GSPO Paper](https://arxiv.org/abs/2507.18071)\n",
    "- [TRL: GRPOTrainer](https://huggingface.co/docs/trl/grpo_trainer)\n",
    "- [Qwen3.5-35B-A3B](https://huggingface.co/Qwen/Qwen3.5-35B-A3B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
