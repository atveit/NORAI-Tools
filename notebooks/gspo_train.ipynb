{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: GSPO Alignment Training on Qwen3.5-35B-A3B\n",
    "\n",
    "This notebook performs **GSPO (Group Sequence Policy Optimization)** alignment training on **Qwen/Qwen3.5-35B-A3B** using the GSPO-prepared Norwegian Alpaca dataset from Phase 1.5.\n",
    "\n",
    "## What is GSPO?\n",
    "\n",
    "GSPO is the RL alignment algorithm developed by the Qwen team ([arXiv:2507.18071](https://arxiv.org/abs/2507.18071)). It improves upon GRPO (DeepSeek's Group Relative Policy Optimization) in two critical ways:\n",
    "\n",
    "1. **Sequence-level importance ratios** instead of token-level — eliminates noise and instability of token-level optimization\n",
    "2. **Inherent MoE stability** — GRPO requires \"Routing Replay\" to stabilize MoE expert routing; GSPO eliminates this dependency entirely\n",
    "\n",
    "In TRL, GSPO is implemented via `GRPOTrainer` with `importance_sampling_level=\"sequence\"`.\n",
    "\n",
    "## Why the Instruct Model?\n",
    "\n",
    "- GSPO is a **policy optimization** method that refines an already-capable policy\n",
    "- The instruct model already follows instructions, giving GSPO meaningful behavior to optimize\n",
    "- Qwen3.5-35B-A3B is a sparse MoE model (35B total, 3B activated) — GSPO was designed for stable MoE RL training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library with GSPO extras\n",
    "# On Colab, clone the repo first:\n",
    "#   !git clone https://github.com/your-username/NORAI-Tools.git /content/NORAI-Tools\n",
    "#   %pip install -e \"/content/NORAI-Tools[gspo]\"\n",
    "\n",
    "%pip install -e \"../[gspo]\"\n",
    "%pip install trl>=0.28.0 peft>=0.15.0 accelerate bitsandbytes\n",
    "\n",
    "from norai_tools import (\n",
    "    semantic_reward,\n",
    "    language_reward,\n",
    "    length_reward,\n",
    "    accuracy_reward,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Configuration — Model, GSPO params, LoRA, reward weights\n",
    "# ============================================================\n",
    "\n",
    "# Model\n",
    "MODEL_NAME = \"Qwen/Qwen3.5-35B-A3B\"\n",
    "\n",
    "# Dataset\n",
    "GSPO_DATASET_PATH = \"norwegian_alpaca_gspo.parquet\"\n",
    "\n",
    "# GSPO-specific settings\n",
    "IMPORTANCE_SAMPLING_LEVEL = \"sequence\"  # Key GSPO setting (vs \"token\" for GRPO)\n",
    "LOSS_TYPE = \"grpo\"\n",
    "BETA = 0.04                             # KL penalty (GSPO paper default)\n",
    "EPSILON = 3e-4                          # Clipping range (tighter than GRPO default)\n",
    "NUM_GENERATIONS = 8                     # Group size G\n",
    "\n",
    "# Training hyperparams\n",
    "LEARNING_RATE = 5e-7\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "MAX_COMPLETION_LENGTH = 512\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "LOGGING_STEPS = 10\n",
    "SAVE_STEPS = 200\n",
    "BF16 = True\n",
    "USE_VLLM = True\n",
    "VLLM_MODE = \"colocate\"\n",
    "\n",
    "# LoRA config\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "\n",
    "# Reward weights: [semantic, language, length, accuracy]\n",
    "REWARD_WEIGHTS = [2.0, 1.0, 0.5, 1.5]\n",
    "\n",
    "# Output\n",
    "OUTPUT_DIR = \"./gspo_qwen_norwegian\"\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"GSPO: importance_sampling_level={IMPORTANCE_SAMPLING_LEVEL}, beta={BETA}, epsilon={EPSILON}\")\n",
    "print(f\"LoRA: r={LORA_R}, alpha={LORA_ALPHA}, targets={LORA_TARGET_MODULES}\")\n",
    "print(f\"Generations per prompt: {NUM_GENERATIONS}\")\n",
    "print(f\"Reward weights: {REWARD_WEIGHTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load GSPO-prepared dataset, verify columns + task distribution\n",
    "# ============================================================\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "dataset = load_dataset(\"parquet\", data_files=GSPO_DATASET_PATH, split=\"train\")\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset)} rows\")\n",
    "print(f\"Columns: {dataset.column_names}\")\n",
    "\n",
    "# Verify required columns\n",
    "required = [\"prompt\", \"task_type\", \"output_improved\"]\n",
    "missing = [c for c in required if c not in dataset.column_names]\n",
    "if missing:\n",
    "    raise ValueError(\n",
    "        f\"Missing columns: {missing}. \"\n",
    "        \"Run Phase 1.5 (prepare_gspo_dataset.ipynb) first.\"\n",
    "    )\n",
    "\n",
    "# Task type distribution\n",
    "task_counts = Counter(dataset[\"task_type\"])\n",
    "print(\"\\nTask type distribution:\")\n",
    "for task, count in sorted(task_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {task:20s}: {count:6d} ({100*count/len(dataset):.1f}%)\")\n",
    "\n",
    "# Show sample\n",
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "display(dataset.select(range(3)).to_pandas()[[\"task_type\", \"output_improved\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Pre-load semantic similarity model + reward summary\n",
    "# ============================================================\n",
    "\n",
    "# Warm up the semantic_reward model with a dummy call so it's\n",
    "# loaded before training starts (avoids timeout on first batch)\n",
    "print(\"Loading semantic similarity model (intfloat/multilingual-e5-large)...\")\n",
    "_ = semantic_reward(\n",
    "    completions=[[{\"content\": \"test\"}]],\n",
    "    output_improved=[\"test\"],\n",
    ")\n",
    "print(\"Similarity model loaded.\")\n",
    "\n",
    "print(\"\\nReward functions:\")\n",
    "names = [\"semantic_reward\", \"language_reward\", \"length_reward\", \"accuracy_reward\"]\n",
    "for name, weight in zip(names, REWARD_WEIGHTS):\n",
    "    print(f\"  {name:20s}  weight={weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Setup LoRA + GRPOConfig with GSPO settings\n",
    "# ============================================================\n",
    "\n",
    "from peft import LoraConfig\n",
    "from trl import GRPOConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    # GSPO-specific\n",
    "    importance_sampling_level=IMPORTANCE_SAMPLING_LEVEL,\n",
    "    loss_type=LOSS_TYPE,\n",
    "    beta=BETA,\n",
    "    epsilon=EPSILON,\n",
    "    # Generation\n",
    "    num_generations=NUM_GENERATIONS,\n",
    "    max_completion_length=MAX_COMPLETION_LENGTH,\n",
    "    # Training\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    bf16=BF16,\n",
    "    # vLLM\n",
    "    use_vllm=USE_VLLM,\n",
    "    vllm_mode=VLLM_MODE,\n",
    "    # Logging/saving\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=3,\n",
    "    report_to=\"none\",  # Set to \"wandb\" for monitoring\n",
    ")\n",
    "\n",
    "print(\"LoRA config:\")\n",
    "print(f\"  r={peft_config.r}, alpha={peft_config.lora_alpha}, dropout={peft_config.lora_dropout}\")\n",
    "print(f\"  Target modules: {peft_config.target_modules}\")\n",
    "print()\n",
    "print(\"GSPO training config:\")\n",
    "print(f\"  importance_sampling_level: {training_args.importance_sampling_level}\")\n",
    "print(f\"  beta={training_args.beta}, epsilon={training_args.epsilon}\")\n",
    "print(f\"  num_generations={training_args.num_generations}\")\n",
    "print(f\"  lr={training_args.learning_rate}, batch={training_args.per_device_train_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Initialize GRPOTrainer and train\n",
    "# ============================================================\n",
    "\n",
    "from trl import GRPOTrainer\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=MODEL_NAME,\n",
    "    reward_funcs=[\n",
    "        semantic_reward,\n",
    "        language_reward,\n",
    "        length_reward,\n",
    "        accuracy_reward,\n",
    "    ],\n",
    "    reward_weights=REWARD_WEIGHTS,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "print(f\"Trainer initialized. Dataset: {len(dataset)} rows\")\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Training complete!\")\n",
    "print(f\"  Total steps: {train_result.global_step}\")\n",
    "print(f\"  Training loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Save LoRA adapter, print metrics, generate samples\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Save adapter\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "print(f\"Model saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# Training metrics\n",
    "metrics = train_result.metrics\n",
    "print(\"\\nTraining metrics:\")\n",
    "for key, value in sorted(metrics.items()):\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Sample Norwegian outputs\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Sample generations:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "eval_prompts = [\n",
    "    \"Forklar hva fotosyntese er med enkle ord.\",\n",
    "    \"Skriv tre fordeler med regelmessig trening.\",\n",
    "    \"Hva er forskjellen mellom vær og klima?\",\n",
    "]\n",
    "\n",
    "for prompt_text in eval_prompts:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(trainer.model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = trainer.model.generate(\n",
    "            **inputs, max_new_tokens=256, do_sample=True,\n",
    "            temperature=0.7, top_p=0.9,\n",
    "        )\n",
    "\n",
    "    generated = tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True\n",
    "    )\n",
    "    print(f\"\\nPrompt: {prompt_text}\")\n",
    "    print(f\"Response: {generated[:300]}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(f\"\\nLoRA adapter saved at: {OUTPUT_DIR}\")\n",
    "print(\"Load with: PeftModel.from_pretrained(base_model, '{}')\".format(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware Requirements & Tips\n",
    "\n",
    "GSPO is more demanding than SFT because it generates G completions per prompt online.\n",
    "\n",
    "| Setup | Configuration | Notes |\n",
    "|-------|---------------|-------|\n",
    "| **Minimum** | 1x A100 80GB | LoRA + 4-bit quant + vLLM colocate, G=4 |\n",
    "| **Recommended** | 2x A100 80GB | LoRA + BF16, vLLM server on 2nd GPU, G=8 |\n",
    "| **Comfortable** | 4x A100 80GB | ZeRO Stage 3 + vLLM, G=16, larger batches |\n",
    "\n",
    "Since Qwen3.5-35B-A3B only activates 3B parameters per token (sparse MoE), memory and compute are much lower than a dense 35B model.\n",
    "\n",
    "## Tips\n",
    "\n",
    "- **4-bit quantization:** Add `load_in_4bit=True` if VRAM is tight and install `bitsandbytes`\n",
    "- **Reduce `num_generations`:** Lower from 8 to 4 if OOM during generation\n",
    "- **Disable vLLM:** Set `USE_VLLM = False` if vLLM causes issues (slower but functional)\n",
    "- **Monitoring:** Set `report_to=\"wandb\"` in GRPOConfig for real-time training curves\n",
    "- **Resume training:** Pass `resume_from_checkpoint=True` to `trainer.train()` if interrupted\n",
    "\n",
    "## GSPO vs GRPO\n",
    "\n",
    "| Aspect | GRPO | GSPO |\n",
    "|--------|------|------|\n",
    "| Importance ratio | Token-level | Sequence-level |\n",
    "| MoE stability | Requires Routing Replay | Inherently stable |\n",
    "| Clipping epsilon | 0.1–0.2 (typical) | 3e-4 (much tighter) |\n",
    "| TRL parameter | `importance_sampling_level=\"token\"` | `importance_sampling_level=\"sequence\"` |\n",
    "\n",
    "## References\n",
    "\n",
    "- [GSPO Paper](https://arxiv.org/abs/2507.18071)\n",
    "- [Qwen Blog: GSPO](https://qwenlm.github.io/blog/gspo/)\n",
    "- [TRL: GRPOTrainer](https://huggingface.co/docs/trl/grpo_trainer)\n",
    "- [Qwen3.5-35B-A3B](https://huggingface.co/Qwen/Qwen3.5-35B-A3B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
