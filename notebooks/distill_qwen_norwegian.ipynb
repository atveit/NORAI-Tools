{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1b: Qwen Distillation + Norwegian Polish\n",
    "\n",
    "This notebook creates a high-quality Norwegian instruction dataset by combining:\n",
    "\n",
    "- **Qwen3.5-35B-A3B** — generates factually rich, well-structured Norwegian answers\n",
    "- **Borealis 4B** — polishes the Norwegian into natural, fluent Bokmål\n",
    "\n",
    "The original Alpaca outputs were generated by gpt-3.5-turbo. Qwen is a much more capable model, so regenerating the outputs gives us better content. But Qwen's Norwegian may still have English-influenced phrasing — Borealis fixes that.\n",
    "\n",
    "## Two-Pass Pipeline\n",
    "\n",
    "The two models can't coexist in memory, so we run sequentially:\n",
    "\n",
    "1. **Pass 1 (Qwen):** Load Qwen → generate Norwegian outputs for all instructions → save intermediate → unload\n",
    "2. **Pass 2 (Borealis):** Load Borealis → polish Qwen's Norwegian → save final dataset\n",
    "\n",
    "Both passes support checkpointing for safe resumption.\n",
    "\n",
    "**Input:** `norwegian_alpaca_improved.parquet` (from Phase 1 — uses the improved instructions/inputs)\n",
    "**Output:** `norwegian_alpaca_qwen_polished.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library\n",
    "# On Colab:\n",
    "#   !git clone https://github.com/your-username/NORAI-Tools.git /content/NORAI-Tools\n",
    "#   %pip install -e /content/NORAI-Tools\n",
    "\n",
    "%pip install -e ..\n",
    "\n",
    "from norai_tools import AlpacaImprover, OUTPUT_FILE\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "\n",
    "# Input\n",
    "IMPROVED_DATASET_PATH = OUTPUT_FILE  # \"norwegian_alpaca_improved.parquet\"\n",
    "\n",
    "# Qwen generation (Pass 1)\n",
    "QWEN_MODEL = \"Qwen/Qwen3.5-35B-A3B\"\n",
    "QWEN_BATCH_SIZE = 4          # Small batches — Qwen is large\n",
    "QWEN_MAX_NEW_TOKENS = 512\n",
    "QWEN_CHECKPOINT = \"qwen_generation_checkpoint.jsonl\"\n",
    "QWEN_INTERMEDIATE = \"norwegian_alpaca_qwen_raw.parquet\"\n",
    "SYSTEM_PROMPT = \"Du er en hjelpsom assistent. Svar alltid på norsk bokmål.\"\n",
    "\n",
    "# Borealis polishing (Pass 2)\n",
    "BOREALIS_BATCH_SIZE = 8\n",
    "BOREALIS_CHECKPOINT = \"borealis_polish_checkpoint.jsonl\"\n",
    "\n",
    "# Output\n",
    "OUTPUT_PATH = \"norwegian_alpaca_qwen_polished.parquet\"\n",
    "PUSH_TO_HUB = False\n",
    "HUB_REPO_ID = \"your-username/norwegian-alpaca-qwen-polished\"\n",
    "\n",
    "print(f\"Qwen model:  {QWEN_MODEL}\")\n",
    "print(f\"Input:       {IMPROVED_DATASET_PATH}\")\n",
    "print(f\"Intermediate:{QWEN_INTERMEDIATE}\")\n",
    "print(f\"Output:      {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load improved dataset from Phase 1\n",
    "# ============================================================\n",
    "\n",
    "dataset = load_dataset(\"parquet\", data_files=IMPROVED_DATASET_PATH, split=\"train\")\n",
    "\n",
    "print(f\"Loaded: {len(dataset)} rows\")\n",
    "print(f\"Columns: {dataset.column_names}\")\n",
    "\n",
    "required = [\"instruction_improved\", \"input_improved\", \"instruction_en\"]\n",
    "missing = [c for c in required if c not in dataset.column_names]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns: {missing}. Run Phase 1 first.\")\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "display(\n",
    "    dataset.select(range(3))\n",
    "    .to_pandas()[[\"instruction_improved\", \"input_improved\"]]\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Pass 1: Generate Norwegian outputs with Qwen\n",
    "# ============================================================\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load checkpoint (resume support)\n",
    "qwen_outputs = []\n",
    "if os.path.exists(QWEN_CHECKPOINT):\n",
    "    with open(QWEN_CHECKPOINT, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                qwen_outputs.append(json.loads(line.strip())[\"output\"])\n",
    "    print(f\"Resumed from checkpoint: {len(qwen_outputs)} rows done.\")\n",
    "\n",
    "resume_idx = len(qwen_outputs)\n",
    "total = len(dataset)\n",
    "\n",
    "if resume_idx >= total:\n",
    "    print(\"Pass 1 already complete!\")\n",
    "else:\n",
    "    print(f\"Generating rows {resume_idx}–{total-1} ({total - resume_idx} remaining)\")\n",
    "    print(f\"Loading {QWEN_MODEL}...\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(QWEN_MODEL)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        QWEN_MODEL,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    model.eval()\n",
    "    print(\"Model loaded.\")\n",
    "\n",
    "    for start in tqdm(range(resume_idx, total, QWEN_BATCH_SIZE), desc=\"Generating\"):\n",
    "        end = min(start + QWEN_BATCH_SIZE, total)\n",
    "        batch = dataset.select(range(start, end))\n",
    "\n",
    "        # Build chat prompts\n",
    "        formatted_prompts = []\n",
    "        for row in batch:\n",
    "            user_content = row[\"instruction_improved\"]\n",
    "            input_text = row.get(\"input_improved\", \"\") or \"\"\n",
    "            if input_text.strip():\n",
    "                user_content += f\"\\n\\n{input_text}\"\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "            ]\n",
    "            formatted = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            formatted_prompts.append(formatted)\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            formatted_prompts, return_tensors=\"pt\",\n",
    "            padding=True, truncation=True, max_length=2048,\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=QWEN_MAX_NEW_TOKENS,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "            )\n",
    "\n",
    "        # Decode generated tokens (everything after the padded input)\n",
    "        padded_len = inputs[\"input_ids\"].shape[1]\n",
    "        batch_texts = []\n",
    "        for i in range(len(formatted_prompts)):\n",
    "            text = tokenizer.decode(outputs[i][padded_len:], skip_special_tokens=True)\n",
    "            batch_texts.append(text)\n",
    "\n",
    "        qwen_outputs.extend(batch_texts)\n",
    "\n",
    "        # Checkpoint\n",
    "        with open(QWEN_CHECKPOINT, \"a\", encoding=\"utf-8\") as f:\n",
    "            for text in batch_texts:\n",
    "                f.write(json.dumps({\"output\": text}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Generation complete: {len(qwen_outputs)} rows\")\n",
    "\n",
    "    # Unload Qwen\n",
    "    del model, tokenizer\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Qwen model unloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Save intermediate + inspect Qwen raw outputs\n",
    "# ============================================================\n",
    "\n",
    "# Merge Qwen outputs into dataset\n",
    "dataset = dataset.add_column(\"output_qwen_raw\", qwen_outputs)\n",
    "dataset.to_parquet(QWEN_INTERMEDIATE)\n",
    "print(f\"Intermediate saved: {QWEN_INTERMEDIATE}\")\n",
    "\n",
    "# Inspect samples\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n",
    "print(\"\\nSample Qwen outputs:\")\n",
    "for i in range(min(3, len(dataset))):\n",
    "    row = dataset[i]\n",
    "    print(f\"\\n--- Row {i} ---\")\n",
    "    print(f\"  Instruction: {row['instruction_improved'][:120]}\")\n",
    "    print(f\"  Original:    {row.get('output_improved', '')[:200]}\")\n",
    "    print(f\"  Qwen raw:    {row['output_qwen_raw'][:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Pass 2: Polish Qwen outputs with Borealis\n",
    "# ============================================================\n",
    "\n",
    "# If restarting from here, load the intermediate parquet\n",
    "if \"output_qwen_raw\" not in dataset.column_names:\n",
    "    dataset = load_dataset(\"parquet\", data_files=QWEN_INTERMEDIATE, split=\"train\")\n",
    "    print(f\"Loaded intermediate: {len(dataset)} rows\")\n",
    "\n",
    "# Load checkpoint (resume support)\n",
    "polished_outputs = []\n",
    "if os.path.exists(BOREALIS_CHECKPOINT):\n",
    "    with open(BOREALIS_CHECKPOINT, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                polished_outputs.append(json.loads(line.strip())[\"output\"])\n",
    "    print(f\"Resumed polish: {len(polished_outputs)} rows done.\")\n",
    "\n",
    "resume_idx = len(polished_outputs)\n",
    "\n",
    "if resume_idx >= len(dataset):\n",
    "    print(\"Pass 2 already complete!\")\n",
    "else:\n",
    "    print(f\"Polishing rows {resume_idx}–{len(dataset)-1} ({len(dataset) - resume_idx} remaining)\")\n",
    "\n",
    "    improver = AlpacaImprover(batch_size=BOREALIS_BATCH_SIZE)\n",
    "    improver.load_model()\n",
    "    print(f\"Borealis loaded: {improver.model_name}\")\n",
    "\n",
    "    for start in tqdm(range(resume_idx, len(dataset), BOREALIS_BATCH_SIZE), desc=\"Polishing\"):\n",
    "        end = min(start + BOREALIS_BATCH_SIZE, len(dataset))\n",
    "\n",
    "        # (qwen_output, english_instruction) — instruction_en gives\n",
    "        # Borealis enough semantic context for language polishing\n",
    "        batch_pairs = [\n",
    "            (dataset[i][\"output_qwen_raw\"], dataset[i][\"instruction_en\"])\n",
    "            for i in range(start, end)\n",
    "        ]\n",
    "        polished = improver.improve_batch(batch_pairs)\n",
    "        polished_outputs.extend(polished)\n",
    "\n",
    "        # Checkpoint\n",
    "        with open(BOREALIS_CHECKPOINT, \"a\", encoding=\"utf-8\") as f:\n",
    "            for text in polished:\n",
    "                f.write(json.dumps({\"output\": text}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Polishing complete: {len(polished_outputs)} rows\")\n",
    "\n",
    "# Add polished column\n",
    "dataset = dataset.add_column(\"output_qwen_polished\", polished_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Compare: Original Alpaca vs Qwen Raw vs Qwen Polished\n",
    "# ============================================================\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Side-by-side comparison\")\n",
    "print(\"=\" * 80)\n",
    "for i in range(min(5, len(dataset))):\n",
    "    row = dataset[i]\n",
    "    print(f\"\\n--- Row {i} ---\")\n",
    "    print(f\"  Instruction:  {row['instruction_improved'][:120]}\")\n",
    "    print(f\"  Original:     {row.get('output_improved', '')[:200]}\")\n",
    "    print(f\"  Qwen raw:     {row['output_qwen_raw'][:200]}\")\n",
    "    print(f\"  Qwen polished:{row['output_qwen_polished'][:200]}\")\n",
    "\n",
    "# Stats: how many outputs changed during polishing?\n",
    "changed = sum(\n",
    "    1 for i in range(len(dataset))\n",
    "    if dataset[i][\"output_qwen_raw\"] != dataset[i][\"output_qwen_polished\"]\n",
    ")\n",
    "print(f\"\\nBorealis changed {changed}/{len(dataset)} outputs ({100*changed/len(dataset):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Save final dataset + optional Hub push\n",
    "# ============================================================\n",
    "\n",
    "dataset.to_parquet(OUTPUT_PATH)\n",
    "print(f\"Saved to: {OUTPUT_PATH}\")\n",
    "print(f\"  Rows: {len(dataset)}\")\n",
    "print(f\"  Columns: {dataset.column_names}\")\n",
    "\n",
    "if PUSH_TO_HUB:\n",
    "    dataset.push_to_hub(HUB_REPO_ID, private=True)\n",
    "    print(f\"Pushed to Hub: {HUB_REPO_ID}\")\n",
    "else:\n",
    "    print(\"\\nSet PUSH_TO_HUB = True and update HUB_REPO_ID to push to the Hub.\")\n",
    "\n",
    "# Cleanup notes\n",
    "print(\"\\nCheckpoint files (safe to delete after verifying output):\")\n",
    "for f in [QWEN_CHECKPOINT, BOREALIS_CHECKPOINT, QWEN_INTERMEDIATE]:\n",
    "    if os.path.exists(f):\n",
    "        print(f\"  {f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
